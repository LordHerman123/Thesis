{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.11.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\AppData\\Local\\Temp\\ipykernel_3828\\2838497400.py:27: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-darkgrid\")\n"
     ]
    }
   ],
   "source": [
    "#Import list\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import xlsxwriter\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import Generative_model\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "\n",
    "#all pymc3 requirement\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import theano as th\n",
    "import arviz as az\n",
    "\n",
    "\n",
    "from pymc3 import Model, Normal, Slice, sample, Uniform, Binomial, HalfNormal\n",
    "from pymc3.distributions import Interpolated\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "print(f\"Running on PyMC3 v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to generate samples\n",
    "#The function will add the results to the previous data set\n",
    "# it will also add the total samples used per degree to a list called n_samples_per_degree\n",
    "def generate_samples(mu,sigma,n_samples,n_samples_per_degree,degree,data_set):\n",
    "    #create normal distribution\n",
    "    normal_dist = stats.norm(loc = mu, scale=sigma)\n",
    "    #sample data given amount of samples and distribution\n",
    "    responses = np.random.binomial(n_samples,p=normal_dist.cdf(degree))\n",
    "    #add responses to data set with +8 shift to account for degrees \n",
    "    data_set[degree+8] += responses\n",
    "    #add amount of samples to n_samples_per_degree\n",
    "    n_samples_per_degree[degree+8] += n_samples\n",
    "    #return file\n",
    "    return data_set,n_samples_per_degree\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this function calculates a cumulative densistiy function based on a given mean and std. This is used to convert the input variables.\n",
    "def cumulative_normal(x, mean, sigma, s=np.sqrt(2)):\n",
    "    # Cumulative distribution function for the standard normal distribution\n",
    "    return 0.5 + 0.5 * tt.erf((x-mean)/(sigma*s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate entropy\n",
    "def Calculate_entropy(lambda_list, theta):\n",
    "    # Initialize lists\n",
    "    # Probability of clockwise\n",
    "    pcw = np.zeros(len(theta))\n",
    "    # Probability of counter clockwise\n",
    "    pccw = np.zeros(len(theta))  # Corrected to create a new zero array instead of sharing the reference\n",
    "\n",
    "    # The dimensions of the input lambdas \n",
    "    dimensions_lambda = np.shape(lambda_list)\n",
    "    \n",
    "    # Initialization of empty list for the probability of a given lambda with a given theta being clockwise\n",
    "    param_theta_cw = np.zeros((dimensions_lambda[0], len(theta)))\n",
    "    param_theta_ccw = np.zeros((dimensions_lambda[0], len(theta)))  # Corrected to create a new zero array instead of sharing the reference\n",
    "\n",
    "    # Go over all theta values\n",
    "    for idx, theta_val in enumerate(theta):     \n",
    "        Pcw_theta_lambda = []\n",
    "        for lamb in lambda_list:\n",
    "            # Given a specific lambda and degree (theta) clockwise probability\n",
    "            Pcw_theta_lambda.append(stats.norm.cdf(theta_val, lamb[0], lamb[1]))\n",
    "        # What is the total probability of clockwise for this degree (theta) \n",
    "        pcw[idx] = sum(Pcw_theta_lambda) / dimensions_lambda[0]\n",
    "        # Probability counterclockwise\n",
    "        pccw[idx] = 1 - pcw[idx]\n",
    "\n",
    "    # Clip lists to avoid NaN\n",
    "    pcw = np.clip(pcw, 1e-6, 1 - 1e-6)\n",
    "    pccw = np.clip(pccw, 1e-6, 1 - 1e-6)\n",
    "\n",
    "    # For each parameter pair\n",
    "    for lamb_idx, lamb in enumerate(lambda_list):      # does this just calculate the mean? \n",
    "        # For each theta_val\n",
    "        for idx, theta_val in enumerate(theta):\n",
    "            # Find given probability\n",
    "            p_cw = stats.norm.cdf(theta_val, lamb[0], lamb[1])\n",
    "            p_ccw = 1 - p_cw\n",
    "            #Calculate probability of a lambda given theta and direction (cw or ccw)\n",
    "            param_theta_cw[lamb_idx, idx] = p_cw / pcw[idx]\n",
    "            param_theta_ccw[lamb_idx, idx] = p_ccw / pccw[idx]\n",
    "\n",
    "    # Calculate the log value of the probabilities\n",
    "    # Clip is included to avoid NaN values\n",
    "    log_param_theta_cw = np.log(np.clip(param_theta_cw, 1e-6, None))\n",
    "    log_param_theta_ccw = np.log(np.clip(param_theta_ccw, 1e-6, None))\n",
    "\n",
    "    # Calculate individual multiplications\n",
    "    pre_sum_cw = log_param_theta_cw * param_theta_cw\n",
    "    pre_sum_ccw = log_param_theta_ccw * param_theta_ccw\n",
    "    # Calculate H (entropy component for cw and ccw)\n",
    "    H_cw = np.sum(pre_sum_cw, axis=0)\n",
    "    H_ccw = np.sum(pre_sum_ccw, axis=0)\n",
    "\n",
    "    # Entropy calculation\n",
    "    entropy = H_cw * pcw + H_ccw * pccw\n",
    "    return entropy,(np.argmax(entropy)-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth\n",
    "mu,sigma = 1,2\n",
    "stimuli = np.arange(-8,9)\n",
    "#set up the data:\n",
    "#number of samples used per iteration\n",
    "samples_per_degree = np.zeros(17)\n",
    "data_set = np.zeros(17)\n",
    "n_samples = 1\n",
    "# for i in range(-8,9):\n",
    "#     data_set,samples_per_degree = generate_samples(mu,sigma,n_samples=1,n_samples_per_degree=samples_per_degree,degree=i,data_set=data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entropy_x -8', 'Entropy_x -7', 'Entropy_x -6', 'Entropy_x -5', 'Entropy_x -4', 'Entropy_x -3', 'Entropy_x -2', 'Entropy_x -1', 'Entropy_x 0', 'Entropy_x 1', 'Entropy_x 2', 'Entropy_x 3', 'Entropy_x 4', 'Entropy_x 5', 'Entropy_x 6', 'Entropy_x 7', 'Entropy_x 8', 'mu', 'sigma', 'stimulus_used', 'lambda_list']\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"Entropy_x \" + str(i) for i in range(-8,9)]\n",
    "column_names.append(\"mu\")\n",
    "column_names.append(\"sigma\")\n",
    "column_names.append(\"stimulus_used\")\n",
    "column_names.append(\"lambda_list\")\n",
    "print(column_names)\n",
    "data_frame_adaptive = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\herma\\anaconda3\\envs\\pymc3_env\\lib\\site-packages\\deprecat\\classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, mn]\n"
     ]
    }
   ],
   "source": [
    "#set up the model:\n",
    "model_cdf = pm.Model()\n",
    "\n",
    "with model_cdf:\n",
    "    curve_mean = pm.Uniform('mn', lower=-8, upper =8)#pm.Normal(\"curve_mu\",mu=0,sigma=5)\n",
    "    curve_std =pm.Gamma(\"sd\",alpha=3,beta=1)#pm.HalfNormal(\"curve_sigma\",sigma=1.5)\n",
    "\n",
    "    #calculate the cdfs of the means and std to fit onto the observations. \n",
    "    theta = pm.Deterministic('theta', cumulative_normal(stimuli,curve_mean,curve_std))\n",
    "    Y_obs = pm.Binomial(\"Y_obs\", n=samples_per_degree, p=theta, observed=data_set)\n",
    "    model_trace = pm.sample(500, cores=4)  # Sampling from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb_test = [(model_trace[\"mn\"][i],model_trace[\"sd\"][i]) for i in range(len(model_trace[\"sd\"]))]   \n",
    "for i in lamb_test:\n",
    "    #plot the line with a 0.01 opacity to ensure viewability\n",
    "    plt.plot(stimuli,stats.norm(loc = i[0], scale=i[1]).cdf(stimuli),alpha=0.01,color=\"r\")\n",
    "#plot ground truth\n",
    "plt.plot(stimuli,stats.norm(loc = mu, scale=sigma).cdf(stimuli),color=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_counter = 0\n",
    "while iteration_counter <20:\n",
    "    #check mean if criteria is met\n",
    "    mu_mean = np.mean(model_trace[\"mn\"])\n",
    "    sd_mean = np.mean(model_trace[\"sd\"])\n",
    "    \n",
    "    \n",
    "    #check if the mu and sigma are within the hitting mark\n",
    "    #extracting posterior samples\n",
    "    lambda_list_adaptive = [(model_trace[\"mn\"][i],model_trace[\"sd\"][i]) for i in range(len(model_trace[\"sd\"]))]\n",
    "\n",
    "    entropy,proposed_stim = Calculate_entropy(lambda_list_adaptive,stimuli)\n",
    "    iteration_counter += 1\n",
    "    print(proposed_stim)\n",
    "    data_set,samples_per_degree = generate_samples(mu,sigma,1,samples_per_degree,proposed_stim,data_set)\n",
    "    print(data_set)\n",
    "    print(samples_per_degree)\n",
    "    \n",
    "    with model_cdf:\n",
    "        #fit the new data to the observations\n",
    "        Y_obs = pm.Binomial(\"Y_obs\"+str(iteration_counter), n=samples_per_degree, p=theta, observed=data_set)\n",
    "        #trace the posterior\n",
    "        model_trace = pm.sample(500, cores=4)   # Sampling from the model\n",
    "    \n",
    "    \n",
    "    data_frame_adaptive.loc[iteration_counter] = entropy.tolist() + [mu_mean, sd_mean,proposed_stim]+[lambda_list_adaptive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lamb_test = [(model_trace[\"mn\"][i],model_trace[\"sd\"][i]) for i in range(len(model_trace[\"sd\"]))]   \n",
    "for i in lamb_test:\n",
    "    #plot the line with a 0.01 opacity to ensure viewability\n",
    "    plt.plot(stimuli,stats.norm(loc = i[0], scale=i[1]).cdf(stimuli),alpha=0.01,color=\"r\")\n",
    "#plot ground truth\n",
    "plt.plot(stimuli,stats.norm(loc = mu, scale=sigma).cdf(stimuli),color=\"b\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
